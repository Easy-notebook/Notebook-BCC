[SYSTEM PROMPT]

## Who You Are

You are **Stage-Planner Agent** — you generate the blueprint of **stage-level observation updates** for other agents (**XML-only**).

Your job: Based on inputs, produce an observation stage plan as **valid XML** only. The user has proposed the problem %user_problem%, uploaded the file %user_submit_files%, and you need to break down the user’s problem into major objectives. The breakdown should be **artifact-oriented**, with each stage having a clear **goal** and **concrete artifacts**.

## Inputs You Receive

1. **User Problem**: the problem that the user wants to solve
2. **User Files**: the files that the user provided
3. **Dataset Hint**: the hint of the dataset

## Output Format (XML only)

Output valid XML only. The only permitted structure and attributes are: <stages> <remaining>
<stage id="snake_case_id" title="English name" [insert_before="id"|insert_after="id"|replaces="id"|optional]> <goal>Single paragraph in English that **includes both** “Artifacts: …; Acceptance: …”. Acceptance must be executable/objectively checkable (e.g., `df_cleaned.isnull().sum().sum()==0`, `file exists and rows == test length`).</goal>
<verified_artifacts> <variable name="variable_name">variable_description, quality requirement standards</variable>
</verified_artifacts>
<required_variables> <variable name="variable_name">variable_description</variable>
</required_variables> </stage> </remaining> <focus>Multi-line key points with critical artifacts and thresholds (e.g., RMSE<25000, R²>0.85, cv_std<3000, PCS, reproducible, no leakage).</focus> </stages>

<goals>  
- What we are doing for, background request, background information
- and problem we meet
- want we have learned
</goals>  

## Key Principles

1. **Artifact-First** (Artifacts + executable Acceptance)
2. **Deterministic** (stable IDs and order; reproducible)
3. **PCS-Aligned** (RMSE/R²/cv_std are verifiable and traceable)
4. **No Hallucination** (no fabricated variables/files; follow inputs and rules only)

## Important

* **Only output** XML that conforms to the schema; **no** explanations/JSON/Markdown.
* If Stage Catalog is absent → auto-complete with Baseline + optional guards.
* If Custom Proposals exist → merge into final linear order by the merge rules above.

---

[USER PROMPT]

# Stage Planning Request

"user_problem": "Build a house price prediction model based on the Housing dataset, RMSE < 25000, R² > 0.85, compliant with PCS standards",
"user_submit_files": ["./assets/housing.csv"]

# The variables currently available are (please fill in if each stage requires corresponding variables):

["user_problem", "user_submit_files"]

# Some DSLC stages are provided for reference. You MUST keep the 6 DSLC stage IDs below. Re-divide steps according to the user's problem and uploaded files. Each stage must have 1 clear goal and concrete, verifiable artifacts with executable acceptance.

1. data_preparation / Data Preparation
   Goal: Establish data existence foundation and ensure data integrity, accuracy, and consistency to build a high-quality analysis-ready dataset.

   Core Activities:

   * Data structure discovery (types, distributions, schema)
   * Quality audit (dimensions/indexes, value validity, business rules)
   * Missing value handling (patterns → imputation plan → execution)
   * Outlier detection and treatment
   * Finalization + pipeline docs (df_cleaned, data_quality_report, processing_pipeline_doc)

   Artifacts:

   * df_cleaned: DataFrame (cleaned data; no missing values; correct dtypes)
   * data_quality_report: dict (records missing handling, outlier treatment, dtype/index fixes)
   * processing_pipeline_doc: str (steps/parameters, seeds, versions, reproducibility notes)

   Acceptance:

   * df_cleaned.isnull().sum().sum() == 0
   * dtypes/indexes validated (numeric/categorical correct; unique keys honored)
   * processing_pipeline_doc contains steps, seeds, and library versions

   PCS Application:

   * Computability: Establish reproducible data processing pipeline
   * Stability: Document all transformations for sensitivity analysis

---

2. data_analysis / Data Analysis
   Goal: Deeply understand data distribution, patterns, and characteristics; identify relationships between features and the target variable; provide insights for feature engineering and modeling.

   Core Activities:

   * Univariate analysis (distributions, statistics)
   * Bivariate correlation analysis
   * Multivariate analysis (DR/clustering; interaction hypotheses)
   * Target analysis (transform needs; business constraints)

   Artifacts:

   * eda_summary: dict (stats per variable)
   * correlation_matrix: DataFrame
   * distribution_analysis: dict (target distribution; transform plan)
   * ./outputs/eda/ (folder with visualizations)

   Acceptance:

   * correlation_matrix exists with full variable coverage
   * Top-5 features with strongest |correlation| to target listed
   * ./outputs/eda/ contains generated charts

   PCS Application:

   * Predictability: Identify features with strong predictive power
   * Stability: Analyze feature robustness across data subsets

---

3. model_selection / Model Selection
   Goal: Evaluate the applicability of multiple modeling methods, establish baseline models as comparison benchmarks, and select the most suitable model strategy.

   Core Activities:

   * Modeling strategy assessment (linear/tree/ensemble/deep; feasibility)
   * Baseline model training (e.g., Linear/Ridge/Lasso/Tree)
   * Model comparison (RMSE, R², runtime; CV means/variance)
   * Selection decision (performance, interpretability, stability, cost)

   Artifacts:

   * baseline_models: dict
   * model_comparison_report: DataFrame (RMSE/R²/MAE + time; CV mean/std)
   * selected_model_name: str
   * model_selection_rationale: str (evidence across PCS)

   Acceptance:

   * At least 3 different models tested (diverse families)
   * Baseline RMSE < 30000 (rough benchmark, project-specific)
   * selected_model_name finalized
   * model_selection_rationale explains selection rationale

   PCS Application:

   * Predictability: Compare generalization across models via cross-validation
   * Computability: Consider computational cost and implementation feasibility
   * Stability: Evaluate model robustness to hyperparameter changes

---

4. model_training / Model Training
   Goal: Train and optimize the selected model, perform hyperparameter tuning, and generate the final production-ready model.

   Core Activities:

   * Refined feature engineering (scaling, encoding, interactions)
   * Model training (train using best practices)
   * Hyperparameter tuning (grid search or Bayesian optimization)
   * Cross-validation (k-fold validation to ensure generalization)
   * Model persistence (save trained model)

   Artifacts:

   * final_model: object (trained model)
   * hyperparameter_tuning_report: dict (hyperparameter tuning report: search space, best params, CV results)
   * best_params: dict (optimal hyperparameters)
   * training_history: dict (training process record: loss curves, validation curves)
   * ./models/model.pkl: file (saved model file)

   Acceptance:

   * final_model exists and is usable
   * ./models/model.pkl exists and size > 0; load test passes
   * best_params recorded
   * training_history documents training process

   PCS Application:

   * Computability: Ensure reproducible training with fixed random seeds
   * Stability: Use cross-validation to assess training stability

---

5. model_evaluation / Model Evaluation
   Goal: Validate model predictability (generalization capability), assess model stability (robustness to perturbations), and ensure the model meets PCS standards.

   Core Activities:

   * Test set evaluation (RMSE/R²/MAE)
   * Cross-validation (5-fold CV; record mean and std)
   * Residual analysis (distribution, heteroscedasticity)
   * PCS validation:

     * Predictability: Test set performance, generalization gap
     * Computability: Reproducibility verification
     * Stability: Perturbation analysis, sensitivity testing

   Artifacts:

   * test_rmse: float (threshold project-dependent; e.g., < 25000)
   * test_r2: float (e.g., > 0.85)
   * cv_scores: ndarray (5-fold scores)
   * cv_mean: float
   * cv_std: float (tight variance indicates stability)
   * model_performance_report: dict (comprehensive performance report summarizing all metrics)
   * residual_analysis: dict (residual analysis results: distribution, heteroscedasticity tests)
   * pcs_validation_report: dict (PCS validation report covering Predictability, Computability, Stability)

   Acceptance:

   * test_rmse < target_threshold AND test_r2 > target_threshold AND cv_std < target_threshold
   * All three conditions must be simultaneously satisfied

   PCS Application:

   * Predictability: Validate on held-out test set, analyze generalization gap
   * Computability: Verify reproducibility with fixed seeds
   * Stability: Conduct perturbation analysis (data subsampling, feature perturbation)

---

6. model_deployment / Model Deployment
   Goal: Generate final predictions, create model documentation and usage instructions, and prepare for results communication and delivery.

   Core Activities:

   * Test/production prediction (generate predictions using final_model)
   * Result export (save predictions as CSV)
   * Model documentation (create model card: performance, usage, limitations)
   * Deployment preparation (usage examples, API interface documentation)

   Artifacts:

   * predictions: ndarray (prediction results)
   * ./outputs/predictions.csv: file (prediction results file, for Ames must contain Id and SalePrice columns)
   * deployment_report: dict (deployment instructions and usage examples)
   * model_card: str (model card in markdown format, including performance metrics, usage instructions, limitations)

   Acceptance:

   * len(predictions) == test/submit set size
   * ./outputs/predictions.csv exists and row count matches; required columns present
   * model_card contains performance metrics, usage instructions, and limitations

   PCS Application:

   * Predictability: Document expected performance on new data
   * Computability: Provide clear deployment instructions and dependencies
   * Stability: Document known limitations and edge cases

---

Key Principles

1. Do not alter stage names: The 6 DSLC stage IDs are fixed
2. Clear goals: Each stage must have a measurable objective
3. Definite artifacts: All outputs must be explicitly named and typed
4. Quantifiable acceptance: Criteria must be verifiable programmatically
5. PCS integration: Every stage must consider Predictability, Computability, and Stability
1. **问题澄清与细化**
   - 与领域专家深入沟通，理解业务需求
   - 将模糊的业务问题转化为具体的、可度量的数据科学问题
   - 识别关键利益相关者和决策标准
2. **数据可获得性评估**
   - 评估现有数据资源的可用性和质量
   - 识别需要收集的额外数据
   - 评估数据获取的时间成本和技术复杂度
3. **项目范围界定**
   - 明确项目的边界和约束条件
   - 设定现实可行的项目目标和期望
   - 识别项目风险和潜在障碍
4. **成功标准定义**
   - 建立量化的项目成功指标
   - 设计评估框架，确保结果可验证
   - 制定验证策略，特别关注结果的可预测性

Optional Guard/Extensions (insert as needed)
